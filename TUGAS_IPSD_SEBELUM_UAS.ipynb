{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "44af080b-a605-4122-9a42-611a527e6f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data telah disimpan ke file: data_group.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Baca file teks hasil eksport\n",
    "file_path = \"data_group.txt\"  # Ubah dengan nama file Anda\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Regex untuk memisahkan tanggal, waktu, pengirim, dan pesan\n",
    "pattern = r\"(\\d{1,2}/\\d{1,2}/\\d{2,4}) (\\d{1,2}\\.\\d{2}) - ([^:]+): (.+)\"\n",
    "\n",
    "# Parsing data\n",
    "data = []\n",
    "for line in lines:\n",
    "    match = re.match(pattern, line)\n",
    "    if match:\n",
    "        date, time, sender, message = match.groups()\n",
    "        data.append({\"Date\": date, \"Time\": time, \"Sender\": sender, \"Message\": message})\n",
    "\n",
    "# Buat DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "csv_file = \"data_group.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Data telah disimpan ke file: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73491826-b916-4ae6-88ba-c01d662456a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beberapa baris pertama dari file yang diekstrak:\n",
      "Date,Time,Sender,Message\n",
      "27/08/22,07.12,+62 838-6608-5648,Pada ngumpul dimana nih?\n",
      "27/08/22,07.23,Tofa IT,Pesan ini dihapus\n",
      "27/08/22,07.23,+62 812-7846-0128,Di anu\n",
      "27/08/22,08.25,+62 813-9016-2761,info sandi wifi edotarium kk?\n",
      "29/08/22,16.29,+62 812-7846-0128,<Media tidak disertakan>\n",
      "29/08/22,21.05,+62 812-7846-0128,ada ingpo pembagian kelompok LPPIK donk\n",
      "29/08/22,21.18,Aldino Ti,Gek ndang satset\n",
      "29/08/22,21.19,+62 838-6608-5648,<Media tidak disertakan>\n",
      "29/08/22,21.39,+62 812-7846-0128,\n",
      "Data telah dibersihkan dan disimpan sementara ke file: data_group_cleaned.csv\n",
      "File hasil telah ditambahkan ke arsip: data_group.tar\n",
      "File sementara data_group_cleaned.csv telah dihapus.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def load_file_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Mencari file di dalam arsip\n",
    "        file = tar.extractfile(file_name)\n",
    "        if file:\n",
    "            return file.read().decode('utf-8')  # Mengembalikan data sebagai string\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan dalam arsip.\")\n",
    "\n",
    "# Tentukan path arsip .tar dan nama file di dalam arsip\n",
    "tar_file_path = \"data_group.tar\"\n",
    "file_name_in_tar = \"data_group.csv\"  # Nama file di dalam arsip\n",
    "\n",
    "# Membaca data dari arsip .tar\n",
    "try:\n",
    "    file_content = load_file_from_tar(tar_file_path, file_name_in_tar)\n",
    "    \n",
    "    # Tampilkan beberapa baris pertama dari file untuk debugging\n",
    "    print(\"Beberapa baris pertama dari file yang diekstrak:\")\n",
    "    print(file_content[:500])  # Menampilkan 500 karakter pertama untuk pengecekan\n",
    "    \n",
    "    # Membaca data sebagai CSV\n",
    "    file_data = StringIO(file_content)\n",
    "    df = pd.read_csv(file_data)\n",
    "\n",
    "    # Debug: Cek apakah kolom 'Message' ada\n",
    "    if 'Message' not in df.columns:\n",
    "        print(\"Kolom 'Message' tidak ditemukan!\")\n",
    "        print(\"Kolom yang tersedia:\", df.columns)\n",
    "    else:\n",
    "        # Fungsi untuk membersihkan teks\n",
    "        def clean_text(text):\n",
    "            if isinstance(text, str):\n",
    "                # Hanya mempertahankan angka, huruf, dan tanda baca-tulis umum\n",
    "                return re.sub(r'[^a-zA-Z0-9.,:;!?()\\-\"\\' ]', '', text)\n",
    "            return text\n",
    "        \n",
    "        # Bersihkan kolom 'Message'\n",
    "        df['Message'] = df['Message'].apply(clean_text)\n",
    "        \n",
    "        # Simpan hasil ke file CSV sementara\n",
    "        output_csv_path = \"data_group_cleaned.csv\"\n",
    "        df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Data telah dibersihkan dan disimpan sementara ke file: {output_csv_path}\")\n",
    "\n",
    "        # Perbarui arsip .tar dengan file CSV hasil\n",
    "        with tarfile.open(tar_file_path, \"a\") as tar:  # Gunakan mode \"a\" untuk menambahkan ke arsip yang ada\n",
    "            tar.add(output_csv_path, arcname=\"data_group_cleaned.csv\")\n",
    "            print(f\"File hasil telah ditambahkan ke arsip: {tar_file_path}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Hapus file sementara setelah digunakan\n",
    "if os.path.exists(output_csv_path):\n",
    "    os.remove(output_csv_path)\n",
    "    print(f\"File sementara {output_csv_path} telah dihapus.\")\n",
    "else:\n",
    "    print(f\"File sementara {output_csv_path} tidak ditemukan untuk dihapus.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6233ecd-9bf0-42c2-bba0-fee996026fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (100, 9)\n",
      "Feature names: ['ada', 'di', 'disertakan', 'info', 'infor', 'ini', 'media', 'tidak', 'turu']\n"
     ]
    }
   ],
   "source": [
    "#scale.py\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def load_yelp_reviews(num_docs):\n",
    "    # Membuka arsip .tar\n",
    "    with tarfile.open('data_group.tar', 'r') as tar:\n",
    "        # Mengakses file data_group_cleaned.csv di dalam arsip\n",
    "        datafile = tar.extractfile('data_group_cleaned.csv')\n",
    "        if datafile is None:\n",
    "            raise FileNotFoundError(\"data_group_cleaned.csv tidak ditemukan dalam arsip data_group.tar\")\n",
    "        \n",
    "        # Membaca file CSV ke dalam DataFrame\n",
    "        df = pd.read_csv(datafile)\n",
    "        \n",
    "        # Memastikan kolom 'Message' ada\n",
    "        if 'Message' not in df.columns:\n",
    "            raise KeyError(\"Kolom 'Message' tidak ditemukan dalam file data_group_cleaned.csv\")\n",
    "        \n",
    "        # Hilangkan baris dengan NaN di kolom 'Message'\n",
    "        df['Message'] = df['Message'].fillna(\"\").astype(str)\n",
    "        \n",
    "        # Mengambil sejumlah dokumen sesuai num_docs\n",
    "        return list(islice(df['Message'], num_docs))\n",
    "\n",
    "def make_matrix(docs, binary=False):\n",
    "    # Modify min_df and max_df values\n",
    "    vec = CountVectorizer(min_df=5, max_df=0.9, binary=binary)  # Adjusted parameters\n",
    "    mtx = vec.fit_transform(docs)\n",
    "    \n",
    "    # Extract column names\n",
    "    cols = [None] * len(vec.vocabulary_)\n",
    "    for word, index in vec.vocabulary_.items():\n",
    "        cols[index] = word\n",
    "    \n",
    "    return mtx, cols\n",
    "\n",
    "# Contoh penggunaan\n",
    "num_docs = 100\n",
    "docs = load_yelp_reviews(num_docs)\n",
    "mtx, cols = make_matrix(docs)\n",
    "\n",
    "print(\"Matrix shape:\", mtx.shape)\n",
    "print(\"Feature names:\", cols[:10])  # Menampilkan 10 fitur pertama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d31dae60-5d31-4ab6-b4ea-8ed3d4f35449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Matrix disimpan ke file: data_group.tar\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "tar_file_path = \"data_group.tar\"\n",
    "\n",
    "scipy.sparse.save_npz('matrix.npz', mtx)\n",
    "pd.DataFrame({'feature': cols}).to_csv('features.csv', index=False)\n",
    "print(f\"Data Matrix disimpan ke file: {tar_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60fc2ac1-b8e7-4815-95e3-50ecb4d449d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def top_words(num_clusters, clusters, mtx, columns):\n",
    "    top = []\n",
    "    for i in range(num_clusters):  # Loop over each cluster\n",
    "        rows_in_cluster = np.where(clusters == i)[0]  # Get rows belonging to the current cluster\n",
    "        word_freqs = mtx[rows_in_cluster].sum(axis=0).A[0]  # Sum word frequencies for the cluster\n",
    "        ordered_freqs = np.argsort(word_freqs)  # Sort frequencies\n",
    "        top_words = [\n",
    "            (columns[idx], int(word_freqs[idx]))  # Get top words and their frequencies\n",
    "            for idx in islice(reversed(ordered_freqs), 20)\n",
    "        ]\n",
    "        top.append(top_words)\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f329d57-9d9e-472c-9b62-36f628ba01f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriks fitur dibuat dengan shape: (100, 329)\n",
      "Contoh kolom fitur: ['00', '0028', '005', '01', '0128', '0268', '05', '06', '07', '0868']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def load_custom_data(file_path, file_name, num_docs):\n",
    "    with tarfile.open(file_path, 'r') as tar:\n",
    "        # Membuka file tertentu di dalam arsip\n",
    "        datafile = tar.extractfile(file_name)\n",
    "        if datafile:\n",
    "            # Mengembalikan baris pertama sejumlah num_docs\n",
    "            return list(islice(datafile, num_docs))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan dalam arsip.\")\n",
    "\n",
    "# Fungsi untuk membuat matriks fitur dari teks\n",
    "def make_matrix(docs, binary=False):\n",
    "    vec = CountVectorizer(min_df=1, max_df=0.9, binary=binary)\n",
    "    mtx = vec.fit_transform(docs)\n",
    "    cols = [None] * len(vec.vocabulary_)\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        cols[idx] = word\n",
    "    return mtx, cols\n",
    "\n",
    "file_path = 'data_group.tar'\n",
    "file_name = 'data_group.csv'\n",
    "num_docs = 100 \n",
    "\n",
    "# Membaca data\n",
    "try:\n",
    "    docs = load_custom_data(file_path, file_name, num_docs)\n",
    "    # Mengubah byte ke string (jika diperlukan)\n",
    "    docs = [doc.decode('utf-8').strip() for doc in docs]\n",
    "\n",
    "    # Membuat matriks fitur\n",
    "    matrix, columns = make_matrix(docs)\n",
    "    print(\"Matriks fitur dibuat dengan shape:\", matrix.shape)\n",
    "    print(\"Contoh kolom fitur:\", columns[:10])\n",
    "except Exception as e:\n",
    "    print(\"Terjadi kesalahan:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b77f2f6-e679-42c1-99be-6f1d01a21550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak data_group_cleaned.csv dari data_group.tar...\n",
      "Data dengan kluster telah disimpan ke file: data_group_clustered.csv\n",
      "File data_group_clustered.csv telah ditambahkan ke arsip data_group.tar.\n",
      "File sementara data_group_clustered.csv telah dihapus.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def extract_csv_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Memeriksa apakah file yang dimaksud ada di dalam arsip\n",
    "        if file_name in tar.getnames():\n",
    "            print(f\"Mengekstrak {file_name} dari {tar_path}...\")\n",
    "            extracted_file = tar.extractfile(file_name)\n",
    "            if extracted_file:\n",
    "                return pd.read_csv(extracted_file)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan di dalam arsip {tar_path}.\")\n",
    "\n",
    "# Path ke file tar dan nama file CSV di dalamnya\n",
    "tar_path = 'data_group.tar'\n",
    "file_name = 'data_group_cleaned.csv'\n",
    "\n",
    "# Membaca file CSV dari arsip .tar\n",
    "try:\n",
    "    data = extract_csv_from_tar(tar_path, file_name)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Pastikan kolom 'Message' ada\n",
    "if 'Message' not in data.columns:\n",
    "    raise KeyError(\"Kolom 'Message' tidak ditemukan di dataset!\")\n",
    "\n",
    "# Step 1: Preprocess the text data (cleaning the \"Message\" column)\n",
    "data['Message'] = data['Message'].fillna(\"\").str.lower()  # Fill NaN and convert to lowercase\n",
    "\n",
    "# Step 2: Convert the \"Message\" column into numerical representation using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Message'])\n",
    "\n",
    "# Step 3: Perform KMeans clustering with 3, 4, and 5 clusters\n",
    "clusters = {}\n",
    "for n_clusters in [3, 4, 5]:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters[n_clusters] = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the dataset for each KMeans run\n",
    "for n_clusters, labels in clusters.items():\n",
    "    data[f'Cluster_{n_clusters}'] = labels\n",
    "\n",
    "# Save the updated dataset to a new CSV file\n",
    "output_file = 'data_group_clustered.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "print(f\"Data dengan kluster telah disimpan ke file: {output_file}\")\n",
    "\n",
    "# Step 4: Add the resulting CSV file to the tar archive\n",
    "def add_file_to_tar(tar_path, file_to_add):\n",
    "    with tarfile.open(tar_path, 'a') as tar:  # Open in append mode\n",
    "        tar.add(file_to_add, arcname=os.path.basename(file_to_add))\n",
    "        print(f\"File {file_to_add} telah ditambahkan ke arsip {tar_path}.\")\n",
    "\n",
    "try:\n",
    "    add_file_to_tar(tar_path, output_file)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan saat menambahkan file ke arsip: {e}\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# Hapus file sementara setelah digunakan\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "    print(f\"File sementara {output_file} telah dihapus.\")\n",
    "else:\n",
    "    print(f\"File sementara {output_file} tidak ditemukan untuk dihapus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dbc118c7-d599-4f7c-a874-dd76a28ab6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak data_group_clustered.csv dari data_group.tar...\n",
      "Kluster 0: [('ini', 161), ('pesan', 95), ('dihapus', 93)]\n",
      "Kluster 1: [('media', 592), ('tidak', 592), ('disertakan', 592)]\n",
      "Kluster 2: [('terimakasih', 8), ('kak', 2), ('sudah', 2)]\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Fungsi untuk membaca file dari arsip .tar\n",
    "def extract_csv_from_tar(tar_path, file_name):\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Memeriksa apakah file yang dimaksud ada di dalam arsip\n",
    "        if file_name in tar.getnames():\n",
    "            print(f\"Mengekstrak {file_name} dari {tar_path}...\")\n",
    "            extracted_file = tar.extractfile(file_name)\n",
    "            if extracted_file:\n",
    "                return pd.read_csv(extracted_file)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{file_name} tidak ditemukan di dalam arsip {tar_path}.\")\n",
    "\n",
    "# Path ke file tar dan nama file CSV di dalamnya\n",
    "tar_path = 'data_group.tar'\n",
    "file_name = 'data_group_clustered.csv'\n",
    "\n",
    "# Membaca file CSV dari arsip .tar\n",
    "try:\n",
    "    data = extract_csv_from_tar(tar_path, file_name)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Pastikan kolom 'Message' dan 'Cluster_3' ada\n",
    "if 'Message' not in data.columns or 'Cluster_3' not in data.columns:\n",
    "    raise KeyError(\"Kolom 'Message' atau 'Cluster_3' tidak ditemukan di dataset!\")\n",
    "\n",
    "# Mendapatkan 3 kata teratas untuk setiap kluster\n",
    "for cluster_id in sorted(data['Cluster_3'].unique()):  # Iterasi berdasarkan kluster\n",
    "    # Mengambil pesan-pesan dari kluster tertentu\n",
    "    cluster_messages = data[data['Cluster_3'] == cluster_id]['Message']\n",
    "    \n",
    "    # Mengonversi pesan menjadi string dan mengganti NaN dengan string kosong\n",
    "    cluster_messages = cluster_messages.fillna(\"\").astype(str)\n",
    "    \n",
    "    # Menggabungkan semua pesan dalam kluster menjadi satu string dan memisahkannya menjadi kata-kata\n",
    "    all_words = \" \".join(cluster_messages).split()\n",
    "    \n",
    "    # Mendapatkan 3 kata teratas menggunakan Counter\n",
    "    most_common_words = Counter(all_words).most_common(3)\n",
    "    \n",
    "    # Menampilkan hasil\n",
    "    print(f\"Kluster {cluster_id}: {most_common_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38d86b24-664e-44a2-a140-3b2712311a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil analisis telah disimpan ke cluster_analysis.csv\n",
      "   Cluster                                   Top Words\n",
      "0        0         ini (162), pesan (95), dihapus (93)\n",
      "1        1  media (592), tidak (592), disertakan (592)\n",
      "2        2         terimakasih (8), kak (2), sudah (2)\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Fungsi untuk membaca file hasil klastering dari tar\n",
    "def read_clustered_data_from_tar(tar_file_path, csv_file_name):\n",
    "    with tarfile.open(tar_file_path, \"r\") as tar:\n",
    "        extracted_file = tar.extractfile(csv_file_name)\n",
    "        if extracted_file:\n",
    "            df = pd.read_csv(extracted_file)\n",
    "            return df\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{csv_file_name} not found in {tar_file_path}\")\n",
    "\n",
    "# Fungsi untuk mendapatkan 3 kata teratas dari setiap kluster\n",
    "def analyze_clusters(data, cluster_column, message_column):\n",
    "    analysis = []\n",
    "    for cluster_id in data[cluster_column].unique():\n",
    "        cluster_messages = data[data[cluster_column] == cluster_id][message_column].fillna(\"\").astype(str)\n",
    "        all_words = \" \".join(cluster_messages).split()\n",
    "        clean_words = [re.sub(r'[^a-zA-Z0-9]', '', word).lower() for word in all_words if len(word) > 1]\n",
    "        most_common_words = Counter(clean_words).most_common(3)\n",
    "        analysis.append({\n",
    "            \"Cluster\": cluster_id,\n",
    "            \"Top Words\": \", \".join([f\"{word} ({count})\" for word, count in most_common_words])\n",
    "        })\n",
    "    return pd.DataFrame(analysis)\n",
    "\n",
    "# File TAR dan nama file hasil klastering di dalamnya\n",
    "tar_file_path = \"data_group.tar\"\n",
    "clustered_file_name = \"data_group_clustered.csv\"\n",
    "\n",
    "# Analisis data hasil klastering\n",
    "try:\n",
    "    df = read_clustered_data_from_tar(tar_file_path, clustered_file_name)\n",
    "    cluster_analysis = analyze_clusters(df, cluster_column=\"Cluster_3\", message_column=\"Message\")\n",
    "\n",
    "    # Simpan hasil analisis ke CSV\n",
    "    analysis_csv_path = \"cluster_analysis.csv\"\n",
    "    cluster_analysis.to_csv(analysis_csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Hasil analisis telah disimpan ke {analysis_csv_path}\")\n",
    "    print(cluster_analysis)\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi kesalahan: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee63332-4cb6-417e-bdb0-950901675f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42066cd-c4a6-42fe-a45b-4837f6aa999c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
